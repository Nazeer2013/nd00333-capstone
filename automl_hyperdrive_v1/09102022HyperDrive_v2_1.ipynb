{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to your workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.41.0 to work with nahmed30-azureml-workspace\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1662999762706
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create compute\n",
        "\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# NOTE: update the cluster name to match the existing cluster\n",
        "# Choose a name for your CPU cluster\n",
        "amlcompute_cluster_name = \"cpu-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n",
        "                                                           #vm_priority = 'lowpriority', # optional\n",
        "                                                           max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
        "# For a more detailed view of current AmlCompute status, use get_status().\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\nSucceeded......................................................................................................................\nAmlCompute wait for completion finished\n\nWait timeout has been reached\nCurrent provisioning state of AmlCompute is \"Succeeded\" and current node count is \"0\"\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1663000365476
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "found = False\n",
        "key = \"UdacityPrjEmailSpamDataSet\"\n",
        "description_text = \"Spam Detection DataSet for Udacity Capstone Proj \"\n",
        "\n",
        "dataset = None\n",
        "if key in ws.datasets.keys(): \n",
        "        found = True\n",
        "        dataset = ws.datasets[key] \n",
        "if found:\n",
        "        dataset\n",
        "        df = dataset.to_pandas_dataframe()\n",
        "\n",
        "df.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "          v1                      v2  \\\ncount   5572                    5572   \nunique     2                    5169   \ntop      ham  Sorry, I'll call later   \nfreq    4825                      30   \n\n                                                  Column3  \\\ncount                                                  50   \nunique                                                 43   \ntop      bt not his girlfrnd... G o o d n i g h t . . .@\"   \nfreq                                                    3   \n\n                      Column4  Column5  \ncount                      12        6  \nunique                     10        5  \ntop      MK17 92H. 450Ppw 16\"  GNT:-)\"  \nfreq                        2        2  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Column3</th>\n      <th>Column4</th>\n      <th>Column5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5572</td>\n      <td>5572</td>\n      <td>50</td>\n      <td>12</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n      <td>5169</td>\n      <td>43</td>\n      <td>10</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>ham</td>\n      <td>Sorry, I'll call later</td>\n      <td>bt not his girlfrnd... G o o d n i g h t . . .@\"</td>\n      <td>MK17 92H. 450Ppw 16\"</td>\n      <td>GNT:-)\"</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>4825</td>\n      <td>30</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1663000369390
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare a training script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "experiment_folder = 'emailspam_training_hyperdrive_09102022_v1'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print('Folder ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Folder ready.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1663000369583
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create  Python script to train the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/emailspam_training_09102022.py\n",
        "\n",
        "# Import libraries\n",
        "import argparse, joblib, os\n",
        "from azureml.core import Run\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import tensorflow as tf\n",
        "import regex as re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pickle\n",
        "import tempfile\n",
        "from tensorflow.keras.models import Sequential, load_model, save_model, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras import models, layers\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# Get script arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Input dataset\n",
        "parser.add_argument(\"--input-data\", type=str, dest='input_data', help='training dataset')\n",
        "\n",
        "#hyperdrive_feature\n",
        "parser.add_argument(\"--hyperdrive_feature\", type=bool, dest='hyperdrive_feature', help='hyperdrive feature')\n",
        "\n",
        "# Hyperparameters\n",
        "parser.add_argument('--units', type=int, default=64, help=\"Number of nodes\")\n",
        "parser.add_argument('--optimizer', type=str, default='adam', help=\"Algorithm of Choice\")\n",
        "\n",
        "# Add arguments to args collection\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Log Hyperparameter values \n",
        "run.log(\"Number of Nodes:\", np.int(args.units))  \n",
        "run.log(\"Algorithm of Choice:\", np.str(args.optimizer))  \n",
        "\n",
        "# load the email spam dataset -- Get the training data from the input\n",
        "print(\"Loading Email Spam Data...\")\n",
        "df = run.input_datasets['training_data'].to_pandas_dataframe() \n",
        "\n",
        "# Cleanup and Prepare Data # Find and eliminate stop words \n",
        "nltk.download('stopwords')\n",
        "stop_words= set(stopwords.words(\"english\"))\n",
        "stop_words.update(['https', 'http', 'amp', 'CO', 't', 'u', 'new', \"I'm\", \"would\"])\n",
        "\n",
        "\n",
        "spam = df.query(\"v1=='spam'\").v2.str.cat(sep=\" \")\n",
        "ham = df.query(\"v1=='ham'\").v2.str.cat(sep=\" \")\n",
        "\n",
        "# convert spam to 1 and ham to 0\n",
        "df = df.replace('spam', 1)\n",
        "df = df.replace('ham', 0)\n",
        "\n",
        "# Clean the text\n",
        "def cleanText(text):\n",
        "    whitespace = re.compile(r\"\\s+\")\n",
        "    web_address = re.compile(r\"(?i)http(s):\\/\\/[a-z0-9.~_\\-\\/]+\")\n",
        "    user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
        "    text = text.replace('.', '')\n",
        "    text = whitespace.sub(' ', text)\n",
        "    text = web_address.sub('', text)\n",
        "    text = user.sub('', text)\n",
        "    text = re.sub(r\"\\[[^()]*\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text)\n",
        "    return text.lower()\n",
        "\n",
        "df.v2 = [cleanText(item) for item in df.v2]\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.oov_token = '<oovToken>'\n",
        "tokenizer.fit_on_texts(df.v2)\n",
        "vocab = tokenizer.word_index\n",
        "vocabCount = len(vocab)+1\n",
        "\n",
        "\n",
        "# Split Train and Test\n",
        "SPLIT = 5000\n",
        "\n",
        "# Split data into training set and test set\n",
        "xTrain = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(df.v2.to_numpy()), padding='pre', maxlen=171)\n",
        "yTrain = df.v1.to_numpy()\n",
        "\n",
        "dim = xTrain.shape[1]\n",
        "xTest = xTrain[SPLIT:]\n",
        "yTest = yTrain[SPLIT:]\n",
        "\n",
        "xTrain = xTrain[:SPLIT]\n",
        "yTrain = yTrain[:SPLIT]\n",
        "\n",
        "# Train a Keras Sequential classification model without the specified hyperparameters\n",
        "print('Training a classification model')\n",
        "\n",
        "#------------------------------------------------------------\n",
        "#model = tf.keras.Sequential()\n",
        "#model.add(tf.keras.layers.Embedding(input_dim=vocabCount+1, output_dim=64, input_length=dim))\n",
        "#model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "#model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocabCount+1, output_dim=64, input_length=dim))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "# for i in range(args.num_layers):\n",
        "model.add(tf.keras.layers.Dense(args.units, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(args.units, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=args.optimizer, metrics=['Accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(xTrain, yTrain, batch_size=32, epochs=100, initial_epoch=6, validation_data=(xTest, yTest))\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(xTest)\n",
        "acc = np.average(y_hat == yTest)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "# y_scores = model.predict_proba(xTest)\n",
        "# auc = roc_auc_score(yTest,y_scores[:,1])\n",
        "# print('AUC: ' + str(auc))\n",
        "# run.log('AUC', np.float(auc))\n",
        "\n",
        "# Save the model in the run outputs\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "joblib.dump(value=model, filename='outputs/emailspam_model09102022.pkl')\n",
        "    \n",
        "\n",
        "run.complete()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing emailspam_training_hyperdrive_09102022_v1/emailspam_training_09102022.py\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll need a Python environment to be hosted on the compute, so let's define that as Conda configuration file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/emailspam_hyperdrive_env_09102022.yml\n",
        "name: batch_environment\n",
        "dependencies:\n",
        "- python=3.8.5\n",
        "- scikit-learn\n",
        "- pandas\n",
        "- numpy\n",
        "- regex\n",
        "- tensorflow\n",
        "- nltk\n",
        "- pip\n",
        "- pip:\n",
        "  - azureml-defaults"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing emailspam_training_hyperdrive_09102022_v1/emailspam_hyperdrive_env_09102022.yml\n"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a hyperparameter tuning experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "from azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# from wordcloud import WordCloud\n",
        "\n",
        "# Create a Python environment for the experiment\n",
        "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/emailspam_hyperdrive_env_09102022.yml\")\n",
        "\n",
        "# Get the training dataset\n",
        "emailspam_ds = ws.datasets.get(\"UdacityPrjEmailSpamDataSet\")\n",
        "\n",
        "hyperdrive_feature = True\n",
        "\n",
        "# Create a script config\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
        "                                script='emailspam_training_09102022.py',\n",
        "                                # Add non-hyperparameter arguments -in this case, the training dataset\n",
        "                                arguments = ['--input-data', emailspam_ds.as_named_input('training_data'),\n",
        "                                '--hyperdrive_feature', hyperdrive_feature],\n",
        "                                environment=hyper_env,\n",
        "                                compute_target = amlcompute_cluster_name)\n",
        "\n",
        "                                \n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "params = RandomParameterSampling( \n",
        "    {\n",
        "    \"--units\": choice(64, 80),\n",
        "    \"--optimizer\": choice('adam', 'sgd')\n",
        "    })\n",
        "\n",
        "# Configure hyperdrive settings\n",
        "hyperdrive = HyperDriveConfig(run_config=script_config, \n",
        "                          hyperparameter_sampling=params, \n",
        "                          policy=None, # No early stopping policy\n",
        "                          primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
        "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
        "                          max_total_runs=24, # Restict the experiment to 48 iterations\n",
        "                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n",
        "\n",
        "# Run the experiment\n",
        "experiment = Experiment(workspace=ws, name='emailspam-hyperdrive-exp-09122022')\n",
        "run = experiment.submit(config=hyperdrive)\n",
        "\n",
        "# Show the status in the notebook as the experiment runs\n",
        "# RunDetails(run).show()\n",
        "run.wait_for_completion()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "{'runId': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca',\n 'target': 'cpu-cluster',\n 'status': 'Completed',\n 'startTimeUtc': '2022-09-12T16:32:51.282442Z',\n 'endTimeUtc': '2022-09-12T16:42:24.857601Z',\n 'services': {},\n 'properties': {'primary_metric_config': '{\"name\":\"Accuracy\",\"goal\":\"maximize\"}',\n  'resume_from': 'null',\n  'runTemplate': 'HyperDrive',\n  'azureml.runsource': 'hyperdrive',\n  'platform': 'AML',\n  'ContentSnapshotId': '6d9948ce-8ece-4472-872e-b267657d1b85',\n  'user_agent': 'python/3.8.5 (Linux-5.4.0-1077-azure-x86_64-with-glibc2.10) msrest/0.6.21 Hyperdrive.Service/1.0.0 Hyperdrive.SDK/core.1.41.0',\n  'space_size': '4',\n  'score': '0.011864883368379871',\n  'best_child_run_id': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_3',\n  'best_metric_status': 'Succeeded',\n  'best_data_container_id': 'dcid.HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_3'},\n 'inputDatasets': [],\n 'outputDatasets': [],\n 'runDefinition': {'configuration': None,\n  'attribution': None,\n  'telemetryValues': {'amlClientType': 'azureml-sdk-train',\n   'amlClientModule': '[Scrubbed]',\n   'amlClientFunction': '[Scrubbed]',\n   'tenantId': 'db05faca-c82a-4b9d-b9c5-0f64b6755421',\n   'amlClientRequestId': '38c6ff64-537e-4f1b-b3dc-cadeafdb5b9c',\n   'amlClientSessionId': '4b226ab8-4ff3-4273-8ef1-9f4452857a9b',\n   'subscriptionId': '16bc73b5-82be-47f2-b5ab-f2373344794c',\n   'estimator': 'NoneType',\n   'samplingMethod': 'RANDOM',\n   'terminationPolicy': 'Default',\n   'primaryMetricGoal': 'maximize',\n   'maxTotalRuns': 24,\n   'maxConcurrentRuns': 2,\n   'maxDurationMinutes': 10080,\n   'vmSize': None},\n  'snapshotId': '6d9948ce-8ece-4472-872e-b267657d1b85',\n  'snapshots': [],\n  'sourceCodeDataReference': None,\n  'parentRunId': None,\n  'dataContainerId': None,\n  'runType': None,\n  'displayName': None,\n  'environmentAssetId': None,\n  'properties': {},\n  'tags': {},\n  'aggregatedArtifactPath': None},\n 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://nahmed30storage.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_6b599155-3c22-48ba-a53a-fe7195d4bbca/azureml-logs/hyperdrive.txt?sv=2019-07-07&sr=b&sig=tw9Bn9aOPxhQJKP7aSZ%2B5uBRAMSmKkfZMmXjO%2Fvj%2BAo%3D&skoid=990e57a2-6b09-4324-b33f-71ef714de994&sktid=db05faca-c82a-4b9d-b9c5-0f64b6755421&skt=2022-09-12T13%3A46%3A26Z&ske=2022-09-13T21%3A56%3A26Z&sks=b&skv=2019-07-07&st=2022-09-12T16%3A33%3A05Z&se=2022-09-13T00%3A43%3A05Z&sp=r'},\n 'submittedBy': 'Nazeer Ahmed'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1663000986438
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determine the best performing run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all child runs, sorted by the primary metric\n",
        "for child_run in run.get_children_sorted_by_primary_metric():\n",
        "    print(child_run)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'run_id': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_3', 'hyperparameters': '{\"--optimizer\": \"adam\", \"--units\": 64}', 'best_primary_metric': 0.011864883368379871, 'status': 'Completed'}\n{'run_id': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_1', 'hyperparameters': '{\"--optimizer\": \"adam\", \"--units\": 80}', 'best_primary_metric': 0.011082449019511956, 'status': 'Completed'}\n{'run_id': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_2', 'hyperparameters': '{\"--optimizer\": \"sgd\", \"--units\": 80}', 'best_primary_metric': 0.0, 'status': 'Completed'}\n{'run_id': 'HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_0', 'hyperparameters': '{\"--optimizer\": \"sgd\", \"--units\": 64}', 'best_primary_metric': 0.0, 'status': 'Completed'}\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1663000986637
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best run, and its metrics and arguments\n",
        "best_run = run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1663000987342
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
        "print('Best Run Id: ', best_run.id)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best Run Id:  HD_6b599155-3c22-48ba-a53a-fe7195d4bbca_3\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1663006851428
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
        "print(' -Arguments:',script_arguments)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " -Accuracy: 0.011864883368379871\n -Arguments: ['--input-data', 'DatasetConsumptionConfig:training_data', '--hyperdrive_feature', 'True', '--optimizer', 'adam', '--units', '64']\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1663002226439
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've found the best run, you can register the model it trained."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "# Register model\n",
        "reg_model = best_run.register_model(model_path='outputs/emailspam_model09102022.pkl', model_name='emailspam_model_09102022',\n",
        "                        tags={'Training context':'Hyperdrive'},\n",
        "                        properties={'Accuracy': best_run_metrics['Accuracy']})"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1663002506771
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "# Combine scoring script & environment in Inference configuration\n",
        "inference_config = InferenceConfig(entry_script=\"09102022/score.py\",\n",
        "                                   environment=hyper_env)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1663008985510
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice.aci import AciWebservice\n",
        "# Set deployment configuration\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
        "                                                       memory_gb = 1)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1663009129493
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
        "service = Model.deploy(workspace = ws,\n",
        "                       name = \"my-emailspam-service\",\n",
        "                       models = [reg_model],\n",
        "                       inference_config = inference_config,\n",
        "                       deployment_config = deployment_config)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1663009290514
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "AciWebservice(workspace=Workspace.create(name='nahmed30-azureml-workspace', subscription_id='16bc73b5-82be-47f2-b5ab-f2373344794c', resource_group='epe-poc-nazeer'), name=my-emailspam-service, image_id=None, image_digest=None, compute_type=ACI, state=Transitioning, scoring_uri=None, tags={}, properties={}, created_by={'userObjectId': 'a8930881-263c-498d-8975-58e6a0c28f2c', 'userPuId': '10032001567EC76C', 'userIdp': None, 'userAltSecId': None, 'userIss': 'https://sts.windows.net/db05faca-c82a-4b9d-b9c5-0f64b6755421/', 'userTenantId': 'db05faca-c82a-4b9d-b9c5-0f64b6755421', 'userName': 'Nazeer Ahmed', 'upn': 'nahmed30@optumcloud.com'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1663009486696
        }
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}