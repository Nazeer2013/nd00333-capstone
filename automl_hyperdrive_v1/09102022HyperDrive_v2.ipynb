{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.44.0 to work with nahmed30-azureml-workspace\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Create compute\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# NOTE: update the cluster name to match the existing cluster\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "# For a more detailed view of current AmlCompute status, use get_status().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>bt not his girlfrnd... G o o d n i g h t . . .@\"</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v1                      v2  \\\n",
       "count   5572                    5572   \n",
       "unique     2                    5169   \n",
       "top      ham  Sorry, I'll call later   \n",
       "freq    4825                      30   \n",
       "\n",
       "                                                  Column3 Column4  Column5  \n",
       "count                                                  50      12        6  \n",
       "unique                                                 43      10        5  \n",
       "top      bt not his girlfrnd... G o o d n i g h t . . .@\"      GE  GNT:-)\"  \n",
       "freq                                                    3       2        2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "found = False\n",
    "key = \"UdacityPrjEmailSpamDataSet\"\n",
    "description_text = \"Spam Detection DataSet for Udacity Capstone Proj \"\n",
    "\n",
    "dataset = None\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "if found:\n",
    "        dataset\n",
    "        df = dataset.to_pandas_dataframe()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "experiment_folder = 'emailspam_training_hyperdrive_09102022_v1'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print('Folder ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create  Python script to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/emailspam_training_09102022.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pickle\n",
    "import tempfile\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras import models, layers\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Input dataset\n",
    "parser.add_argument(\"--input-data\", type=str, dest='input_data', help='training dataset')\n",
    "\n",
    "#hyperdrive_feature\n",
    "parser.add_argument(\"--hyperdrive_feature\", type=bool, dest='hyperdrive_feature', help='hyperdrive feature')\n",
    "\n",
    "# Hyperparameters\n",
    "parser.add_argument('--units', type=int, default=64, help=\"Number of nodes\")\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help=\"Algorithm of Choice\")\n",
    "\n",
    "# Add arguments to args collection\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Log Hyperparameter values \n",
    "run.log(\"Number of Nodes:\", np.int(args.units))  \n",
    "run.log(\"Algorithm of Choice:\", np.str(args.optimizer))  \n",
    "\n",
    "# load the email spam dataset -- Get the training data from the input\n",
    "print(\"Loading Email Spam Data...\")\n",
    "df = run.input_datasets['training_data'].to_pandas_dataframe() \n",
    "\n",
    "# Cleanup and Prepare Data # Find and eliminate stop words \n",
    "nltk.download('stopwords')\n",
    "stop_words= set(stopwords.words(\"english\"))\n",
    "stop_words.update(['https', 'http', 'amp', 'CO', 't', 'u', 'new', \"I'm\", \"would\"])\n",
    "\n",
    "\n",
    "spam = df.query(\"v1=='spam'\").v2.str.cat(sep=\" \")\n",
    "ham = df.query(\"v1=='ham'\").v2.str.cat(sep=\" \")\n",
    "\n",
    "# convert spam to 1 and ham to 0\n",
    "df = df.replace('spam', 1)\n",
    "df = df.replace('ham', 0)\n",
    "\n",
    "# Clean the text\n",
    "def cleanText(text):\n",
    "    whitespace = re.compile(r\"\\s+\")\n",
    "    web_address = re.compile(r\"(?i)http(s):\\/\\/[a-z0-9.~_\\-\\/]+\")\n",
    "    user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
    "    text = text.replace('.', '')\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = web_address.sub('', text)\n",
    "    text = user.sub('', text)\n",
    "    text = re.sub(r\"\\[[^()]*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "df.v2 = [cleanText(item) for item in df.v2]\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.oov_token = '<oovToken>'\n",
    "tokenizer.fit_on_texts(df.v2)\n",
    "vocab = tokenizer.word_index\n",
    "vocabCount = len(vocab)+1\n",
    "\n",
    "\n",
    "# Split Train and Test\n",
    "SPLIT = 5000\n",
    "\n",
    "# Split data into training set and test set\n",
    "xTrain = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(df.v2.to_numpy()), padding='pre', maxlen=171)\n",
    "yTrain = df.v1.to_numpy()\n",
    "\n",
    "dim = xTrain.shape[1]\n",
    "xTest = xTrain[SPLIT:]\n",
    "yTest = yTrain[SPLIT:]\n",
    "\n",
    "xTrain = xTrain[:SPLIT]\n",
    "yTrain = yTrain[:SPLIT]\n",
    "\n",
    "# Train a Keras Sequential classification model without the specified hyperparameters\n",
    "print('Training a classification model')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#model = tf.keras.Sequential()\n",
    "#model.add(tf.keras.layers.Embedding(input_dim=vocabCount+1, output_dim=64, input_length=dim))\n",
    "#model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "#model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=vocabCount+1, output_dim=64, input_length=dim))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "# for i in range(args.num_layers):\n",
    "model.add(tf.keras.layers.Dense(args.units, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(args.units, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=args.optimizer, metrics=['Accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(xTrain, yTrain, batch_size=32, epochs=100, initial_epoch=6, validation_data=(xTest, yTest))\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(xTest)\n",
    "acc = np.average(y_hat == yTest)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "# y_scores = model.predict_proba(xTest)\n",
    "# auc = roc_auc_score(yTest,y_scores[:,1])\n",
    "# print('AUC: ' + str(auc))\n",
    "# run.log('AUC', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/emailspam_model09102022.pkl')\n",
    "    \n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a Python environment to be hosted on the compute, so let's define that as Conda configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/emailspam_hyperdrive_env_09102022.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.8.5\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- regex\n",
    "- tensorflow\n",
    "- nltk\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/emailspam_hyperdrive_env_09102022.yml\")\n",
    "\n",
    "# Get the training dataset\n",
    "emailspam_ds = ws.datasets.get(\"UdacityPrjEmailSpamDataSet\")\n",
    "\n",
    "hyperdrive_feature = True\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='emailspam_training_09102022.py',\n",
    "                                # Add non-hyperparameter arguments -in this case, the training dataset\n",
    "                                arguments = ['--input-data', emailspam_ds.as_named_input('training_data'),\n",
    "                                '--hyperdrive_feature', hyperdrive_feature],\n",
    "                                environment=hyper_env,\n",
    "                                compute_target = amlcompute_cluster_name)\n",
    "\n",
    "                                \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "params = RandomParameterSampling( \n",
    "    {\n",
    "    \"--units\": choice(16, 32, 48, 64, 80),\n",
    "    \"--optimizer\": choice('adam', 'sgd', 'rmsprop', 'adadelta')\n",
    "    })\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=None, # No early stopping policy\n",
    "                          primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=24, # Restict the experiment to 48 iterations\n",
    "                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='emailspam-hyperdrive-exp-09102022')\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "# RunDetails(run).show()\n",
    "run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the best performing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've found the best run, you can register the model it trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_run.register_model(model_path='outputs/emailspam_model09102022.pkl', model_name='emailspam_model_09102022',\n",
    "                        tags={'Training context':'Hyperdrive'},\n",
    "                        properties={'Accuracy': best_run_metrics['Accuracy']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
