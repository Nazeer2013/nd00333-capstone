# This file has been autogenerated by version 1.45.0 of the Azure Automated Machine Learning SDK.


import numpy
import numpy as np
import pandas as pd
import pickle
import argparse


from azureml.training.tabular._diagnostics import logging_utilities


def setup_instrumentation():
    import logging
    import sys

    from azureml.core import Run
    from azureml.telemetry import INSTRUMENTATION_KEY, get_telemetry_log_handler
    from azureml.telemetry._telemetry_formatter import ExceptionFormatter

    logger = logging.getLogger("azureml.training.tabular")

    try:
        logger.setLevel(logging.INFO)

        # Add logging to STDOUT
        stdout_handler = logging.StreamHandler(sys.stdout)
        logger.addHandler(stdout_handler)

        # Add telemetry logging with formatter to strip identifying info
        telemetry_handler = get_telemetry_log_handler(
            instrumentation_key=INSTRUMENTATION_KEY, component_name="azureml.training.tabular"
        )
        telemetry_handler.setFormatter(ExceptionFormatter())
        logger.addHandler(telemetry_handler)

        # Attach run IDs to logging info for correlation if running inside AzureML
        try:
            run = Run.get_context()
            parent_run = run.parent
            return logging.LoggerAdapter(logger, extra={
                "properties": {
                    "codegen_run_id": run.id,
                    "parent_run_id": parent_run.id
                }
            })
        except Exception:
            pass
    except Exception:
        pass

    return logger


logger = setup_instrumentation()


def split_dataset(X, y, weights, split_ratio, should_stratify):
    from sklearn.model_selection import train_test_split

    random_state = 42
    if should_stratify:
        stratify = y
    else:
        stratify = None

    if weights is not None:
        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
        weights_train, weights_test = None, None

    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)


def get_training_dataset(dataset_id):
    from azureml.core.dataset import Dataset
    from azureml.core.run import Run
    
    logger.info("Running get_training_dataset")
    ws = Run.get_context().experiment.workspace
    dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)
    return dataset.to_pandas_dataframe()


def prepare_data(dataframe):
    from azureml.training.tabular.preprocessing import data_cleaning
    
    logger.info("Running prepare_data")
    label_column_name = 'v1'
    
    # extract the features, target and sample weight arrays
    y = dataframe[label_column_name].values
    X = dataframe.drop([label_column_name], axis=1)
    sample_weights = None
    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,
     is_timeseries=False, target_column=label_column_name)
    
    return X, y, sample_weights


def get_mapper_369e16(column_names):
    from azureml.training.tabular.featurization.text.stringcast_transformer import StringCastTransformer
    from numpy import float32
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn_pandas.dataframe_mapper import DataFrameMapper
    from sklearn_pandas.features_generator import gen_features
    
    definition = gen_features(
        columns=column_names,
        classes=[
            {
                'class': StringCastTransformer,
            },
            {
                'class': TfidfVectorizer,
                'analyzer': 'char',
                'binary': False,
                'decode_error': 'strict',
                'dtype': numpy.float32,
                'encoding': 'utf-8',
                'input': 'content',
                'lowercase': True,
                'max_df': 0.95,
                'max_features': None,
                'min_df': 1,
                'ngram_range': (3, 3),
                'norm': 'l2',
                'preprocessor': None,
                'smooth_idf': True,
                'stop_words': None,
                'strip_accents': None,
                'sublinear_tf': False,
                'token_pattern': '(?u)\\b\\w\\w+\\b',
                'tokenizer': None,
                'use_idf': False,
                'vocabulary': None,
            },
        ]
    )
    mapper = DataFrameMapper(features=definition, input_df=True, sparse=True)
    
    return mapper
    
    
def get_mapper_2cf0a8(column_names):
    from azureml.training.tabular.featurization.text.stringcast_transformer import StringCastTransformer
    from numpy import float32
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn_pandas.dataframe_mapper import DataFrameMapper
    from sklearn_pandas.features_generator import gen_features
    
    definition = gen_features(
        columns=column_names,
        classes=[
            {
                'class': StringCastTransformer,
            },
            {
                'class': TfidfVectorizer,
                'analyzer': 'word',
                'binary': False,
                'decode_error': 'strict',
                'dtype': numpy.float32,
                'encoding': 'utf-8',
                'input': 'content',
                'lowercase': True,
                'max_df': 1.0,
                'max_features': None,
                'min_df': 1,
                'ngram_range': (1, 2),
                'norm': 'l2',
                'preprocessor': None,
                'smooth_idf': True,
                'stop_words': None,
                'strip_accents': None,
                'sublinear_tf': False,
                'token_pattern': '(?u)\\b\\w\\w+\\b',
                'tokenizer': None,
                'use_idf': False,
                'vocabulary': None,
            },
        ]
    )
    mapper = DataFrameMapper(features=definition, input_df=True, sparse=True)
    
    return mapper
    
    
def generate_data_transformation_config():
    from sklearn.pipeline import FeatureUnion
    
    column_group_1 = ['v2', 'Column4', 'Column5', 'Column6']
    
    feature_union = FeatureUnion([
        ('mapper_369e16', get_mapper_369e16(column_group_1)),
        ('mapper_2cf0a8', get_mapper_2cf0a8(column_group_1)),
    ])
    return feature_union
    
    
def generate_preprocessor_config_0():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_0():
    from sklearn.linear_model import SGDClassifier
    
    algorithm = SGDClassifier(
        alpha=6.53064693877551,
        average=False,
        class_weight='balanced',
        early_stopping=False,
        epsilon=0.1,
        eta0=0.01,
        fit_intercept=True,
        l1_ratio=0.3061224489795918,
        learning_rate='constant',
        loss='modified_huber',
        max_iter=1000,
        n_iter_no_change=5,
        n_jobs=1,
        penalty='none',
        power_t=0.5555555555555556,
        random_state=None,
        shuffle=True,
        tol=0.0001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_1():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_1():
    from sklearn.linear_model import SGDClassifier
    
    algorithm = SGDClassifier(
        alpha=3.8776122448979593,
        average=False,
        class_weight='balanced',
        early_stopping=False,
        epsilon=0.1,
        eta0=0.01,
        fit_intercept=False,
        l1_ratio=0.8163265306122448,
        learning_rate='invscaling',
        loss='modified_huber',
        max_iter=1000,
        n_iter_no_change=5,
        n_jobs=1,
        penalty='none',
        power_t=0.2222222222222222,
        random_state=None,
        shuffle=True,
        tol=0.001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_2():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_2():
    from sklearn.linear_model import SGDClassifier
    
    algorithm = SGDClassifier(
        alpha=7.5510448979591835,
        average=False,
        class_weight='balanced',
        early_stopping=False,
        epsilon=0.1,
        eta0=0.001,
        fit_intercept=True,
        l1_ratio=0.42857142857142855,
        learning_rate='constant',
        loss='modified_huber',
        max_iter=1000,
        n_iter_no_change=5,
        n_jobs=1,
        penalty='none',
        power_t=0.7777777777777777,
        random_state=None,
        shuffle=True,
        tol=0.0001,
        validation_fraction=0.1,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_3():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_3():
    from sklearn.linear_model import LogisticRegression
    
    algorithm = LogisticRegression(
        C=719.6856730011514,
        class_weight=None,
        dual=False,
        fit_intercept=True,
        intercept_scaling=1,
        l1_ratio=None,
        max_iter=100,
        multi_class='multinomial',
        n_jobs=1,
        penalty='l2',
        random_state=None,
        solver='lbfgs',
        tol=0.0001,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_4():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_4():
    from sklearn.linear_model import LogisticRegression
    
    algorithm = LogisticRegression(
        C=339.3221771895323,
        class_weight=None,
        dual=False,
        fit_intercept=True,
        intercept_scaling=1,
        l1_ratio=None,
        max_iter=100,
        multi_class='ovr',
        n_jobs=1,
        penalty='l2',
        random_state=None,
        solver='saga',
        tol=0.0001,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_5():
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=False,
        with_std=False
    )
    
    return preproc
    
    
def generate_algorithm_config_5():
    from sklearn.ensemble import RandomForestClassifier
    
    algorithm = RandomForestClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight=None,
        criterion='entropy',
        max_depth=None,
        max_features=0.1,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.01,
        min_weight_fraction_leaf=0.0,
        n_estimators=50,
        n_jobs=1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_6():
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=False,
        with_std=True
    )
    
    return preproc
    
    
def generate_algorithm_config_6():
    from azureml.training.tabular.models.calibrated_model import CalibratedModel
    from sklearn.linear_model import SGDClassifier
    
    algorithm = CalibratedModel(
        base_estimator=SGDClassifier(
            alpha=1.6327367346938775,
            average=False,
            class_weight='balanced',
            early_stopping=False,
            epsilon=0.1,
            eta0=0.01,
            fit_intercept=False,
            l1_ratio=0.8571428571428571,
            learning_rate='constant',
            loss='hinge',
            max_iter=1000,
            n_iter_no_change=5,
            n_jobs=1,
            penalty='none',
            power_t=0.2222222222222222,
            random_state=None,
            shuffle=True,
            tol=0.001,
            validation_fraction=0.1,
            verbose=0,
            warm_start=False
        ),
        random_state=None
    )
    
    return algorithm
    
    
def generate_algorithm_config_meta():
    from azureml.training.tabular.models.stack_ensemble import Scorer
    from sklearn.linear_model import LogisticRegressionCV
    
    algorithm = LogisticRegressionCV(
        Cs=10,
        class_weight=None,
        cv=None,
        dual=False,
        fit_intercept=True,
        intercept_scaling=1.0,
        l1_ratios=None,
        max_iter=100,
        multi_class='auto',
        n_jobs=None,
        penalty='l2',
        random_state=None,
        refit=True,
        scoring=Scorer(
            metric='accuracy'
        ),
        solver='lbfgs',
        tol=0.0001,
        verbose=0
    )
    
    return algorithm
    
    
def generate_algorithm_config():
    from azureml.training.tabular.models.stack_ensemble import StackEnsembleClassifier
    from sklearn.linear_model import LogisticRegressionCV
    from sklearn.pipeline import Pipeline
    
    meta_learner = generate_algorithm_config_meta()
    
    pipeline_0 = Pipeline(steps=[('preproc', generate_preprocessor_config_0()), ('model', generate_algorithm_config_0())])
    pipeline_1 = Pipeline(steps=[('preproc', generate_preprocessor_config_1()), ('model', generate_algorithm_config_1())])
    pipeline_2 = Pipeline(steps=[('preproc', generate_preprocessor_config_2()), ('model', generate_algorithm_config_2())])
    pipeline_3 = Pipeline(steps=[('preproc', generate_preprocessor_config_3()), ('model', generate_algorithm_config_3())])
    pipeline_4 = Pipeline(steps=[('preproc', generate_preprocessor_config_4()), ('model', generate_algorithm_config_4())])
    pipeline_5 = Pipeline(steps=[('preproc', generate_preprocessor_config_5()), ('model', generate_algorithm_config_5())])
    pipeline_6 = Pipeline(steps=[('preproc', generate_preprocessor_config_6()), ('model', generate_algorithm_config_6())])
    algorithm = StackEnsembleClassifier(
        base_learners=[
            ('model_0', pipeline_0),
            ('model_1', pipeline_1),
            ('model_2', pipeline_2),
            ('model_3', pipeline_3),
            ('model_4', pipeline_4),
            ('model_5', pipeline_5),
            ('model_6', pipeline_6),
        ],
        meta_learner=meta_learner,
        training_cv_folds=5
    )
    
    return algorithm
    
    
def generate_pipeline_with_ytransformer(pipeline):
    from azureml.training.tabular.models.pipeline_with_ytransformations import PipelineWithYTransformations
    from sklearn.preprocessing import LabelEncoder
    
    transformer = LabelEncoder()
    transformer_name = "LabelEncoder"
    return PipelineWithYTransformations(pipeline, transformer_name, transformer)
    
def build_model_pipeline():
    from sklearn.pipeline import Pipeline
    
    logger.info("Running build_model_pipeline")
    pipeline = Pipeline(
        steps=[
            ('featurization', generate_data_transformation_config()),
            ('stackensemble', generate_algorithm_config()),
        ]
    )
    
    return generate_pipeline_with_ytransformer(pipeline)


def train_model(X, y, sample_weights=None, transformer=None):
    logger.info("Running train_model")
    model_pipeline = build_model_pipeline()
    
    model = model_pipeline.fit(X, y)
    return model


def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):
    from azureml.training.tabular.score.scoring import score_classification
    
    y_pred_probs = model.predict_proba(X_test)
    if isinstance(y_pred_probs, pd.DataFrame):
        y_pred_probs = y_pred_probs.values
    class_labels = np.unique(y)
    train_labels = model.classes_
    metrics = score_classification(
        y_test, y_pred_probs, get_metrics_names(), class_labels, train_labels, use_binary=True)
    return metrics
def get_metrics_names():
    metrics_names = [
        'average_precision_score_macro',
        'average_precision_score_weighted',
        'weighted_accuracy',
        'precision_score_macro',
        'AUC_weighted',
        'f1_score_binary',
        'recall_score_binary',
        'balanced_accuracy',
        'log_loss',
        'f1_score_weighted',
        'average_precision_score_micro',
        'recall_score_weighted',
        'AUC_binary',
        'precision_score_classwise',
        'average_precision_score_classwise',
        'confusion_matrix',
        'accuracy_table',
        'recall_score_classwise',
        'iou_micro',
        'iou_classwise',
        'iou_macro',
        'AUC_macro',
        'norm_macro_recall',
        'precision_score_weighted',
        'precision_score_binary',
        'accuracy',
        'f1_score_macro',
        'iou',
        'average_precision_score_binary',
        'precision_score_micro',
        'matthews_correlation',
        'AUC_micro',
        'f1_score_classwise',
        'AUC_classwise',
        'iou_weighted',
        'f1_score_micro',
        'recall_score_macro',
        'classification_report',
        'recall_score_micro',
    ]
    return metrics_names


def main(training_dataset_id=None):
    from azureml.core.run import Run
    
    # The following code is for when running this code as part of an AzureML script run.
    run = Run.get_context()
    
    df = get_training_dataset(training_dataset_id)
    X, y, sample_weights = prepare_data(df)
    split_ratio = 0.25
    try:
        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=True)
    except Exception:
        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=False)
    model = train_model(X_train, y_train, sample_weights_train)
    
    metrics = calculate_metrics(model, X, y, sample_weights, X_test=X_valid, y_test=y_valid)
    
    print(metrics)
    for metric in metrics:
        run.log(metric, metrics[metric])
    
    with open('model.pkl', 'wb') as f:
        pickle.dump(model, f)
    run.upload_file('outputs/model.pkl', 'model.pkl')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--training_dataset_id', type=str, default='3527a22f-75c2-4ae0-81f9-28549e60c632', help='Default training dataset id is populated from the parent run')
    args = parser.parse_args()
    
    try:
        main(args.training_dataset_id)
    except Exception as e:
        logging_utilities.log_traceback(e, logger)
        raise